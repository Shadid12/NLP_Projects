{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shadid12/NLP_Projects/blob/main/improved_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nNuJln-M00kd"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install -U \"datasets>=2.14.6\"\n",
        "!pip install \"fsspec==2023.9.2\"\n",
        "!pip install tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-dfbIfS1ET7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_seq_len: int = 512):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create matrix of shape [max_seq_len, d_model]\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        position = torch.arange(0, max_seq_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "\n",
        "        # Apply sin to even indices in the array; 2i\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # Apply cos to odd indices in the array; 2i+1\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Register as a buffer (not a parameter, but moves with model.to(device))\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # shape [1, max_seq_len, d_model]\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: Tensor of shape [batch_size, seq_len, d_model]\n",
        "        returns: same shape, with positional encoding added\n",
        "        \"\"\"\n",
        "        seq_len = x.size(1)\n",
        "        return x + self.pe[:, :seq_len]\n",
        "\n",
        "\n",
        "class TransformerInputEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int, max_seq_len: int = 512):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        input_ids: Tensor of shape [batch_size, seq_len]\n",
        "        returns: Tensor of shape [batch_size, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        token_emb = self.token_embedding(input_ids)\n",
        "        return self.positional_encoding(token_emb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mi0nqgfs2FgP"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"code_search_net\", \"javascript\", download_mode=\"force_redownload\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLigt9N-2UvT"
      },
      "outputs": [],
      "source": [
        " # a Byte-Level BPE tokenizer\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "def code_iterator():\n",
        "    for row in ds['train']:      # ds is an IterableDataset; lazy → low RAM\n",
        "        yield row[\"func_code_string\"]\n",
        "\n",
        "# Make byte pair encodings\n",
        "tokenizer.train_from_iterator(\n",
        "    code_iterator(),\n",
        "    vocab_size=32_000,                 # typical sweet-spot for code\n",
        "    min_frequency=2,\n",
        "    special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SL9aTCNz2bcx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"js-bpe\")\n",
        "tokenizer.save_model(\"js-bpe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEue9rwd_V9q"
      },
      "source": [
        "## Improved Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5KthST3_TMF"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GPTBlock(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, 4 * d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * d_model, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pre-norm architecture (more stable)\n",
        "        seq_len = x.size(1)\n",
        "        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(x.device)\n",
        "\n",
        "        # Self-attention with causal mask\n",
        "        attn_out, _ = self.attn(\n",
        "            self.ln1(x), self.ln1(x), self.ln1(x),\n",
        "            attn_mask=causal_mask\n",
        "        )\n",
        "        x = x + self.dropout(attn_out)\n",
        "\n",
        "        # MLP\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTStyleTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6, max_seq_len=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = TransformerInputEmbedding(vocab_size, d_model, max_seq_len)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.blocks = nn.ModuleList([GPTBlock(d_model, nhead, dropout) for _ in range(num_layers)])\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x = self.embedding(input_ids)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwhu4hgh_iqP"
      },
      "source": [
        "## improved code dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXCVA4sk2AEz"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ImprovedCodeDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, seq_len=256, min_length=50):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_len = seq_len\n",
        "        self.samples = []\n",
        "\n",
        "        # Filter and preprocess texts\n",
        "        processed_texts = []\n",
        "        for text in texts:\n",
        "            # Clean the code\n",
        "            cleaned = self.clean_code(text)\n",
        "            if len(cleaned.strip()) > min_length:  # Filter out very short snippets\n",
        "                processed_texts.append(cleaned)\n",
        "\n",
        "        print(f\"Processed {len(processed_texts)} code samples\")\n",
        "\n",
        "        # Create training samples with sliding window\n",
        "        for text in processed_texts:\n",
        "            token_ids = tokenizer.encode(text).ids\n",
        "\n",
        "            # Skip if too short\n",
        "            if len(token_ids) < seq_len + 1:\n",
        "                continue\n",
        "\n",
        "            # Create overlapping windows (stride = seq_len // 2 for more data)\n",
        "            stride = seq_len // 2\n",
        "            for i in range(0, len(token_ids) - seq_len, stride):\n",
        "                input_ids = token_ids[i:i + seq_len]\n",
        "                target_ids = token_ids[i + 1:i + 1 + seq_len]\n",
        "\n",
        "                if len(input_ids) == seq_len and len(target_ids) == seq_len:\n",
        "                    self.samples.append((input_ids, target_ids))\n",
        "\n",
        "        print(f\"Created {len(self.samples)} training samples\")\n",
        "\n",
        "    def clean_code(self, code):\n",
        "        \"\"\"Clean and normalize JavaScript code\"\"\"\n",
        "        # Remove excessive whitespace\n",
        "        code = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', code)  # Max 2 consecutive newlines\n",
        "        code = re.sub(r' +', ' ', code)  # Multiple spaces to single space\n",
        "\n",
        "        # Normalize common patterns\n",
        "        code = code.replace('\\t', '    ')  # Tabs to 4 spaces\n",
        "\n",
        "        # Remove comments that are too long (they don't help with code prediction)\n",
        "        code = re.sub(r'//.*?(?=\\n|$)', '', code)  # Single line comments\n",
        "        code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)  # Multi-line comments\n",
        "\n",
        "        return code.strip()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_ids, target_ids = self.samples[idx]\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "            \"labels\": torch.tensor(target_ids, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# Create dataset with better preprocessing\n",
        "def create_improved_dataset():\n",
        "    # Load more data if available\n",
        "    texts = []\n",
        "    for split in ['train']:  # Add 'validation' if available\n",
        "        for row in ds[split]:\n",
        "            texts.append(row[\"func_code_string\"])\n",
        "\n",
        "    # Remove duplicates and very short snippets\n",
        "    unique_texts = list(set(texts))\n",
        "    print(f\"Unique code samples: {len(unique_texts)}\")\n",
        "\n",
        "    dataset = ImprovedCodeDataset(unique_texts, tokenizer, seq_len=256)\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtQEdjIYC2iC"
      },
      "source": [
        "### Encode Decode and Helper functins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7cWf-GRCu8G"
      },
      "outputs": [],
      "source": [
        "from typing import List, Sequence, Union\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    \"js-bpe/vocab.json\",\n",
        "    \"js-bpe/merges.txt\",\n",
        ")\n",
        "\n",
        "\n",
        "def encode(\n",
        "    text: Union[str, Sequence[str]],\n",
        "    add_special_tokens: bool = True,\n",
        ") -> Union[List[int], List[List[int]]]:\n",
        "    \"\"\"\n",
        "    Convert raw JavaScript (str) → list[int] token IDs.\n",
        "    Accepts a single string or an iterable of strings (batch).\n",
        "    \"\"\"\n",
        "    if isinstance(text, str):\n",
        "        return tokenizer.encode(text, add_special_tokens=add_special_tokens).ids\n",
        "    # Batch mode\n",
        "    return [\n",
        "        enc.ids for enc in tokenizer.encode_batch(\n",
        "            list(text), add_special_tokens=add_special_tokens\n",
        "        )\n",
        "    ]\n",
        "\n",
        "def decode(\n",
        "    ids: Union[Sequence[int], Sequence[Sequence[int]]],\n",
        "    skip_special_tokens: bool = True,\n",
        ") -> Union[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Convert token IDs back to a JavaScript string.\n",
        "    Accepts a single list[int] or a batch of them.\n",
        "    \"\"\"\n",
        "    # Detect batch vs single\n",
        "    if ids and isinstance(ids[0], (list, tuple)):\n",
        "        return [tokenizer.decode(seq, skip_special_tokens=skip_special_tokens) for seq in ids]  # type: ignore[arg-type]\n",
        "    return tokenizer.decode(ids, skip_special_tokens=skip_special_tokens)  # type: ignore[arg-type]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4yGdHBUC5wX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def top_k_top_p_filtering(logits, top_k=50, top_p=0.9, filter_value=-float('Inf')):\n",
        "    \"\"\"Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\"\"\"\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    return logits\n",
        "\n",
        "def improved_generate(model, prompt: str, max_new_tokens: int = 50,\n",
        "                     temperature: float = 0.8, top_k: int = 50,\n",
        "                     top_p: float = 0.9, repetition_penalty: float = 1.1):\n",
        "    \"\"\"\n",
        "    Improved generation with better sampling strategies\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "\n",
        "    # Encode the prompt\n",
        "    input_ids = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
        "    original_length = input_ids.size(1)\n",
        "\n",
        "    # Keep track of generated tokens for repetition penalty\n",
        "    generated_tokens = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for step in range(max_new_tokens):\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids)\n",
        "            next_token_logits = outputs[0, -1, :] / temperature\n",
        "\n",
        "            # Apply repetition penalty\n",
        "            if repetition_penalty != 1.0 and generated_tokens:\n",
        "                for token_id in set(generated_tokens):\n",
        "                    next_token_logits[token_id] /= repetition_penalty\n",
        "\n",
        "            # Apply top-k and top-p filtering\n",
        "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "\n",
        "            # Sample from the filtered distribution\n",
        "            probs = F.softmax(filtered_logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append to sequence\n",
        "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=-1)\n",
        "            generated_tokens.append(next_token.item())\n",
        "\n",
        "            # Stop if we hit end token\n",
        "            if next_token.item() == tokenizer.token_to_id(\"</s>\"):\n",
        "                break\n",
        "\n",
        "            # Truncate input if it gets too long (sliding window)\n",
        "            if input_ids.size(1) > 512:  # Adjust based on your max_seq_len\n",
        "                input_ids = input_ids[:, 1:]  # Remove first token\n",
        "\n",
        "    # Decode the result\n",
        "    generated_text = decode(input_ids[0].tolist())\n",
        "    return generated_text\n",
        "\n",
        "# Alternative: Beam search for more deterministic generation\n",
        "def beam_search_generate(model, prompt: str, max_new_tokens: int = 50,\n",
        "                        num_beams: int = 4, temperature: float = 1.0):\n",
        "    \"\"\"\n",
        "    Beam search generation for more coherent but less diverse output\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "\n",
        "    # Encode prompt\n",
        "    input_ids = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
        "    batch_size = 1\n",
        "\n",
        "    # Initialize beams\n",
        "    beams = [(input_ids, 0.0)]  # (sequence, score)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for step in range(max_new_tokens):\n",
        "            candidates = []\n",
        "\n",
        "            for seq, score in beams:\n",
        "                if seq[0, -1].item() == tokenizer.token_to_id(\"</s>\"):\n",
        "                    candidates.append((seq, score))\n",
        "                    continue\n",
        "\n",
        "                # Get next token probabilities\n",
        "                outputs = model(seq)\n",
        "                next_token_logits = outputs[0, -1, :] / temperature\n",
        "                next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "                # Get top k candidates\n",
        "                top_probs, top_indices = torch.topk(next_token_probs, num_beams)\n",
        "\n",
        "                for prob, idx in zip(top_probs, top_indices):\n",
        "                    new_seq = torch.cat([seq, idx.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
        "                    new_score = score + torch.log(prob).item()\n",
        "                    candidates.append((new_seq, new_score))\n",
        "\n",
        "            # Select top beams\n",
        "            beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:num_beams]\n",
        "\n",
        "    # Return best sequence\n",
        "    best_seq = beams[0][0]\n",
        "    return decode(best_seq[0].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUCoklzG_oAo"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vGwUMSnCpFN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqdSv7UZ_liw"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from itertools import islice\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "\n",
        "# Prepare data - Use the dataset class, not the model class\n",
        "texts = [row[\"func_code_string\"] for row in islice(ds[\"train\"], 15000)]\n",
        "# Use ImprovedCodeDataset (the dataset class) instead of ImprovedTransformer (the model class)\n",
        "dataset = ImprovedCodeDataset(texts, tokenizer, seq_len=128)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Model and training - Create the model properly\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "model = GPTStyleTransformer(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=512,\n",
        "    nhead=8,\n",
        "    num_layers=4,\n",
        "    max_seq_len=128,\n",
        "    dropout=0.1\n",
        ").to(\"cuda\")\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=3e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(\"cuda\")        # [B, T]\n",
        "        labels = batch[\"labels\"].to(\"cuda\")              # [B, T]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids)                        # [B, T, V]\n",
        "        loss = loss_fn(logits.view(-1, vocab_size), labels.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss = {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# Test generation\n",
        "model.eval()\n",
        "test_prompts = [\n",
        "    \"function add(a, b) {\",\n",
        "    \"const arr = [1, 2, 3];\",\n",
        "    \"if (condition) {\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    result = improved_generate(model, prompt, max_new_tokens=30)\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Generated: {result}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7TOYevjeX2FQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIj8UQIpNpKk"
      },
      "source": [
        "## Saving Model for later inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFWZbejIBagT"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "print(\"Saving the model...\")\n",
        "\n",
        "# 1. Create a directory to save the model\n",
        "output_dir = \"./gpt_js_model\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# 2. Save the model's state_dict\n",
        "model_save_path = os.path.join(output_dir, \"pytorch_model.bin\")\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "# 3. Save the model's configuration\n",
        "model_config = {\n",
        "    \"vocab_size\": model.lm_head.out_features,\n",
        "    \"d_model\": model.embedding.token_embedding.embedding_dim,\n",
        "    \"nhead\": model.blocks[0].attn.num_heads,\n",
        "    \"num_layers\": len(model.blocks),\n",
        "    \"max_seq_len\": model.embedding.positional_encoding.pe.size(1),\n",
        "    \"dropout\": model.dropout.p,\n",
        "}\n",
        "config_save_path = os.path.join(output_dir, \"config.json\")\n",
        "with open(config_save_path, 'w') as f:\n",
        "    json.dump(model_config, f)\n",
        "\n",
        "# 4. The tokenizer is already saved in the \"js-bpe\" directory.\n",
        "#    You can optionally copy it to your model directory for a self-contained package.\n",
        "import shutil\n",
        "tokenizer_dir = os.path.join(output_dir, \"tokenizer\")\n",
        "if os.path.exists(tokenizer_dir):\n",
        "    shutil.rmtree(tokenizer_dir) # remove if it exists\n",
        "shutil.copytree(\"js-bpe\", tokenizer_dir)\n",
        "\n",
        "\n",
        "print(f\"Model, config, and tokenizer saved in {output_dir}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMJdwfrez7H8IoCnm0EWn6h",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}